[2021-12-26 19:04:20,745] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_aluraonline 2021-12-25T09:00:00+00:00 [queued]>
[2021-12-26 19:04:20,758] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_aluraonline 2021-12-25T09:00:00+00:00 [queued]>
[2021-12-26 19:04:20,758] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2021-12-26 19:04:20,759] {taskinstance.py:881} INFO - Starting attempt 2 of 2
[2021-12-26 19:04:20,759] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2021-12-26 19:04:20,772] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_aluraonline> on 2021-12-25T09:00:00+00:00
[2021-12-26 19:04:20,775] {standard_task_runner.py:54} INFO - Started process 36953 to run task
[2021-12-26 19:04:20,811] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'twitter_dag', 'transform_twitter_aluraonline', '2021-12-25T09:00:00+00:00', '--job_id', '19', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/twitter_dag.py', '--cfg_path', '/tmp/tmpq49j8dc2']
[2021-12-26 19:04:20,812] {standard_task_runner.py:78} INFO - Job 19: Subtask transform_twitter_aluraonline
[2021-12-26 19:04:20,853] {logging_mixin.py:112} INFO - Running <TaskInstance: twitter_dag.transform_twitter_aluraonline 2021-12-25T09:00:00+00:00 [running]> on host note-cristian-agriness
[2021-12-26 19:04:20,891] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2021-12-26 19:04:20,894] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/cristian/Desktop/Data_Science/Projects/datapipeline/spark-3.2.0-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/cristian/Desktop/Data_Science/Projects/datapipeline/spark/transformation.py --src /home/cristian/Desktop/Data_Science/Projects/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2021-12-25 --dest /home/cristian/Desktop/Data_Science/Projects/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2021-12-25
[2021-12-26 19:04:21,907] {spark_submit_hook.py:479} INFO - 21/12/26 19:04:21 WARN Utils: Your hostname, note-cristian-agriness resolves to a loopback address: 127.0.1.1; using 192.168.15.9 instead (on interface enp3s0)
[2021-12-26 19:04:21,908] {spark_submit_hook.py:479} INFO - 21/12/26 19:04:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2021-12-26 19:04:21,931] {spark_submit_hook.py:479} INFO - WARNING: An illegal reflective access operation has occurred
[2021-12-26 19:04:21,931] {spark_submit_hook.py:479} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/cristian/Desktop/Data_Science/Projects/datapipeline/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2021-12-26 19:04:21,931] {spark_submit_hook.py:479} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2021-12-26 19:04:21,931] {spark_submit_hook.py:479} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2021-12-26 19:04:21,931] {spark_submit_hook.py:479} INFO - WARNING: All illegal access operations will be denied in a future release
[2021-12-26 19:04:22,194] {spark_submit_hook.py:479} INFO - python3: can't open file '/home/cristian/Desktop/Data_Science/Projects/datapipeline/spark/transformation.py': [Errno 2] No such file or directory
[2021-12-26 19:04:22,198] {spark_submit_hook.py:479} INFO - log4j:WARN No appenders could be found for logger (org.apache.spark.util.ShutdownHookManager).
[2021-12-26 19:04:22,198] {spark_submit_hook.py:479} INFO - log4j:WARN Please initialize the log4j system properly.
[2021-12-26 19:04:22,199] {spark_submit_hook.py:479} INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
[2021-12-26 19:04:22,254] {taskinstance.py:1150} ERROR - Cannot execute: /home/cristian/Desktop/Data_Science/Projects/datapipeline/spark-3.2.0-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/cristian/Desktop/Data_Science/Projects/datapipeline/spark/transformation.py --src /home/cristian/Desktop/Data_Science/Projects/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2021-12-25 --dest /home/cristian/Desktop/Data_Science/Projects/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2021-12-25. Error code is: 2.
Traceback (most recent call last):
  File "/home/cristian/Desktop/datapipeline/.env/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/cristian/Desktop/datapipeline/.env/lib/python3.6/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/cristian/Desktop/datapipeline/.env/lib/python3.6/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 405, in submit
    self._mask_cmd(spark_submit_cmd), returncode
airflow.exceptions.AirflowException: Cannot execute: /home/cristian/Desktop/Data_Science/Projects/datapipeline/spark-3.2.0-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/cristian/Desktop/Data_Science/Projects/datapipeline/spark/transformation.py --src /home/cristian/Desktop/Data_Science/Projects/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2021-12-25 --dest /home/cristian/Desktop/Data_Science/Projects/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2021-12-25. Error code is: 2.
[2021-12-26 19:04:22,255] {taskinstance.py:1194} INFO - Marking task as FAILED. dag_id=twitter_dag, task_id=transform_twitter_aluraonline, execution_date=20211225T090000, start_date=20211226T220420, end_date=20211226T220422
[2021-12-26 19:04:25,719] {local_task_job.py:102} INFO - Task exited with return code 1
